{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BT4103 Data Cleaning\n",
    "Please read through and let me know if there are any issues with regard to the cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install pandas scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages and datasets\n",
    "I have imported all the packages that I used up here for ease of reference. Please add your own filepath so that you can import the data correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import re\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing dataset\n",
    "Add filepath below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(\"oots-cleaned-unlocked.xlsx\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Helper Functions\n",
    "Here are the helper functions that I have created for easier readability in the actual code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OHE(df, col, drop_first=False): #One Hot Encode a column in a df\n",
    "    dummies = pd.get_dummies(df[col], prefix=col, drop_first=drop_first)\n",
    "    df = pd.concat([df.drop(columns=[col]), dummies], axis=1)\n",
    "    legend = {new_col: category \n",
    "              for new_col, category in zip(dummies.columns, dummies.columns.str.replace(f\"{col}_\", \"\", regex=False))}\n",
    "    return df, legend\n",
    "\n",
    "\n",
    "def LabelEncode(df, col): #Label Encode a column in a df\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col].astype(str))\n",
    "    legend = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "    return df, legend\n",
    "\n",
    "def inspect_column(df, col, top_n=20): #Get Unique Value information on column in df\n",
    "    print(f\"Column: {col}\")\n",
    "    print(f\"Unique values: {df[col].nunique(dropna=False)}\")\n",
    "    print(\"\\nTop value counts:\")\n",
    "    print(df[col].value_counts(dropna=False).head(top_n))\n",
    "\n",
    "def apply_mapping(df, col, mapping, new_col_suffix=\"_clean\"): #Apply new mapping to column in df\n",
    "    new_col = col + new_col_suffix\n",
    "    df[new_col] = df[col].replace(mapping)\n",
    "    return df\n",
    "\n",
    "def normalize_text(df, col, to_lower=True, replace_symbols=True, unknown_vals=None): #Normalise text of a column in a df\n",
    "    s = df[col].astype(str).str.strip()\n",
    "    if to_lower:\n",
    "        s = s.str.lower()\n",
    "    s = s.str.replace(r\"\\s+\", \" \", regex=True)\n",
    "    if replace_symbols:\n",
    "        s = s.str.replace(r\"[-_/]\", \" \", regex=True)\n",
    "    if unknown_vals:\n",
    "        s = s.replace(unknown_vals, \"0\")\n",
    "    df[col] = s\n",
    "    return df\n",
    "\n",
    "def _normalize_text_series(s: pd.Series) -> pd.Series: #Normalise text in series\n",
    "    s = s.astype(str).map(lambda x: unicodedata.normalize(\"NFKC\", x))\n",
    "    s = s.str.strip()\n",
    "    s = s.str.lower()\n",
    "    s = s.str.replace(r\"\\s+\", \" \", regex=True)         \n",
    "    s = s.str.replace(r\"\\s*/\\s*\", \" / \", regex=True)   \n",
    "    s = s.str.replace(r\"\\s*,\\s*\", \", \", regex=True)    \n",
    "    s = s.str.strip(\" ,\")                              \n",
    "    return s\n",
    "\n",
    "def _remove_trailing_code_in_parens(name_s: pd.Series, code_s: pd.Series) -> pd.Series: #remove white spaces in ()\n",
    "    code_up = code_s.astype(str).str.strip().str.upper()\n",
    "    pattern = r\"\\(\\s*{}\\s*\\)\\s*$\"\n",
    "    out = name_s.copy()\n",
    "    mask = code_up.notna() & code_up.ne(\"\")\n",
    "    out.loc[mask] = [\n",
    "        re.sub(pattern.format(re.escape(c)), \"\", n, flags=re.IGNORECASE)\n",
    "        for n, c in zip(out.loc[mask].tolist(), code_up.loc[mask].tolist())\n",
    "    ]\n",
    "    return out.str.strip(\" ,\")\n",
    "\n",
    "def _choose_canonical_name(name_series: pd.Series) -> str: #Chooses best name\n",
    "    s = name_series.dropna().astype(str)\n",
    "    s = s[s.str.strip().ne(\"\").values]\n",
    "    s = s[s.str.strip().ne(\"unknown\").values]\n",
    "    if s.empty:\n",
    "        return \"0\"\n",
    "    vc = s.value_counts()\n",
    "    top_freq = vc.iloc[0]\n",
    "    candidates = vc[vc.eq(top_freq)].index.tolist()\n",
    "    return max(candidates, key=len)\n",
    "\n",
    "def build_operation_legend_and_drop_nature( #Nature cleaning\n",
    "    df: pd.DataFrame,\n",
    "    code_col: str = \"OPERATION_CODE\",\n",
    "    nature_col: str = \"NATURE\",\n",
    "    drop_nature: bool = True,\n",
    "    keep_title_case_copy: bool = False,\n",
    "    unknown_tokens = (\"0\", \"na\", \"n/a\", \"-\", \"null\", \"nan\")\n",
    "):\n",
    "    if code_col not in df.columns or nature_col not in df.columns:\n",
    "        raise KeyError(f\"Expected columns '{code_col}' and '{nature_col}' in df.\")\n",
    "    work = pd.DataFrame({\n",
    "        \"operation_code\": df[code_col].astype(str).str.strip().str.upper(),\n",
    "        \"operation_name_raw\": df[nature_col]\n",
    "    })\n",
    "    name_norm = _normalize_text_series(work[\"operation_name_raw\"])\n",
    "    name_norm = name_norm.replace(list(unknown_tokens), \"unknown\")\n",
    "    name_clean = _remove_trailing_code_in_parens(name_norm, work[\"operation_code\"])\n",
    "    tmp = pd.DataFrame({\"operation_code\": work[\"operation_code\"], \"operation_name\": name_clean})\n",
    "    tmp = tmp[tmp[\"operation_code\"].str.len() > 0]\n",
    "    legend = (\n",
    "        tmp.groupby(\"operation_code\", as_index=False)[\"operation_name\"]\n",
    "           .apply(_choose_canonical_name)\n",
    "           .rename(columns={\"operation_name\": \"operation_name\"})\n",
    "    )\n",
    "    if keep_title_case_copy:\n",
    "        legend[\"operation_name_title\"] = legend[\"operation_name\"].str.title()\n",
    "    df_out = df.copy()\n",
    "    if drop_nature:\n",
    "        df_out.drop(columns=[nature_col], inplace=True, errors=\"ignore\")\n",
    "    return df_out, legend\n",
    "\n",
    "  \n",
    "def clean_equipment( #EQUIPMENT cleaning\n",
    "    df,\n",
    "    col=\"EQUIPMENT\",\n",
    "    sep=\";\",\n",
    "    tags_to_strip=(r\"#nuh\",),            \n",
    "    unknown_vals=(\"0\",\"na\",\"n/a\",\"-\",\"null\",\"nan\",\"\"),\n",
    "    synonym_map=None         \n",
    "):\n",
    "    if synonym_map is None:\n",
    "        synonym_map = {}\n",
    "    pattern = r\"|\".join(fr\"{re.escape(tag)}[_-]?\" for tag in tags_to_strip)\n",
    "    df[col] = (\n",
    "        df[col]\n",
    "        .astype(str)\n",
    "        .str.replace(pattern, \"\", regex=True, flags=re.IGNORECASE)\n",
    "    )\n",
    "    df = normalize_text(df, col, to_lower=True, replace_symbols=True, unknown_vals=unknown_vals)\n",
    "    def _clean_token(tok: str) -> str:\n",
    "        t = tok.strip()\n",
    "        if not t: return \"\"\n",
    "        t = re.sub(r\"\\s+\", \" \", t)\n",
    "        t = synonym_map.get(t, t)\n",
    "        if t in (\"unknown\",): return \"\"\n",
    "        return t\n",
    "    def _process_cell(cell: str) -> str:\n",
    "        parts = re.split(rf\"\\s*{re.escape(sep)}\\s*\", cell) if cell else []\n",
    "        cleaned = [_clean_token(p) for p in parts]\n",
    "        cleaned = [c for c in cleaned if c]\n",
    "        if not cleaned:\n",
    "            return \"unknown\"\n",
    "        cleaned = sorted(set(cleaned))\n",
    "        return f\"{sep} \".join(cleaned)\n",
    "    df[col] = df[col].apply(_process_cell)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Cleaning\n",
    "We are dropping index and case number as they are labels, and dropping patient name because it has been completely removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(columns=\"INDEX\")\n",
    "data = data.drop(columns=\"PATIENT_NAME\")\n",
    "data = data.drop(columns=\"CASE_NUMBER\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensure correct data types\n",
    "yet to do, still waiting on update from joey regarding the dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle Missing Data\n",
    "Handling missing data is important to ensure that we do not run into any issues with the EDA, as well as our AI/ML implementations. It is a vital step in data cleaning to ensure that the dataset can be used efficiently and properly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling optional columns\n",
    "These columns potentially will be blank as there is nothing to write for some operations, hence we will fill those with 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Delay_Reason\"].fillna(0, inplace=True)\n",
    "data[\"Remarks\"].fillna(0, inplace=True)\n",
    "data[\"IMPLANT\"].fillna(0, inplace=True)\n",
    "data[\"EQUIPMENT\"].fillna(0, inplace=True)\n",
    "data[\"EMERGENCY_PRIORITY\"].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling missing date/time columns\n",
    "These columns are missing certain timestamps. Hence, for those with missing timestamps, we will fill in \"1900-01-01 00:00:00\" so that we can maintain the date/time format while showing that it is obviously missing because it is in year 1900."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_cols = [c for c in data.columns if \"TIME\" in c.upper()]\n",
    "extra_time_cols = [\n",
    "    \"PLANNED_ANAESTHESIA_INDUCTION\",\n",
    "    \"ACTUAL_ANAESTHESIA_INDUCTION\",\n",
    "    \"PLANNED_SKIN_CLOSURE\",\n",
    "    \"ACTUAL_SKIN_CLOSURE\",\n",
    "    \"BOOKING_DATE\"\n",
    "]\n",
    "\n",
    "time_cols = list(set(time_cols + extra_time_cols))\n",
    "\n",
    "placeholder = \"1900-01-01 00:00:00\"\n",
    "data[time_cols] = data[time_cols].fillna(placeholder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle admission related columns\n",
    "Some of these surgeries may be day surgeries of from the A&E, hence might not have admission data. Hence, we will replace blanks with \"Not Admitted\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "admission_cols = [\"ADMISSION_STATUS\", \"ADMISSION_CLASS_TYPE\", \n",
    "                  \"ADMISSION_TYPE\", \"ADMISSION_WARD\", \"ADMISSION_BED\"]\n",
    "data[admission_cols] = data[admission_cols].fillna(\"Not Admitted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill in missing staff data\n",
    "Some surgeries are missing surgeon, anaesthetist, or diagnosis data, hence we will fill it with \"Unknown\" and \"Not Recorded\". This is because it is likely not possible for a surgery to proceed without them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clinician_cols = [\"SURGEON\", \"ANAESTHETIST_TEAM\", \"ANAESTHETIST_MCR_NO\"]\n",
    "data[clinician_cols] = data[clinician_cols].fillna(\"Unknown\")\n",
    "data[\"DIAGNOSIS\"] = data[\"DIAGNOSIS\"].fillna(\"Not Recorded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop remaining missing rows\n",
    "After filling in the missing values that we are able to fill, there are some columns that are still missing data. We will thus drop them as they make up a very small portion of our overall data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View current state of dataframe\n",
    "Currently, the dataset no longer contains any missing data, and thus we are able to proceed with the next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle Duplicate Data\n",
    "Important to remove to prevent bias in our AI/ML solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for duplicate rows\n",
    "This is to see if our dataset contains any rows that are completely identical. This means that the same surgery has been accidentally logged twice. We want to avoid having this in our dataset as it would cause our analysis in the future to skew."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop duplicate rows\n",
    "We identified 3 duplicate rows, and hence we will want to drop them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep cleaning each column\n",
    "Looking into each individual column to clean up most of the free text portions. Please add more cleaning as we go, as there is quite alot to sieve through and I dont think i caught it all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect Location\n",
    "No problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unique values analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_column(data, \"LOCATION\", top_n=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect Room\n",
    "No problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unique values analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_column(data, \"ROOM\", top_n=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect case status\n",
    "No problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unique values analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_column(data, \"CASE_STATUS\", top_n=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect OPERATION_TYPE\n",
    "No problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_column(data, \"OPERATION_TYPE\", top_n=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect Emergency Priority\n",
    "No problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_column(data, \"EMERGENCY_PRIORITY\", top_n=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect Patient Code\n",
    "No problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_column(data, \"PATIENT_CODE\", top_n=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect Nature\n",
    "Removed this column entirely, and created a legend(can be found below) to map SURGICAL_CODE to NATURE, as they are the same thing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_column(data, \"NATURE\", top_n=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, nature_legend = build_operation_legend_and_drop_nature(\n",
    "    data,\n",
    "    code_col=\"SURGICAL_CODE\",\n",
    "    nature_col=\"NATURE\",\n",
    "    drop_nature=True,           \n",
    "    keep_title_case_copy=True    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.columns)              \n",
    "nature_legend.drop(columns='operation_name_title', inplace=True)\n",
    "nature_legend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect Surgical Code\n",
    "Extension of NATURE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_column(data, \"SURGICAL_CODE\", top_n=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect discipline\n",
    "No problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_column(data, \"DISCIPLINE\", top_n=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect Surgeon\n",
    "No problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_column(data, \"SURGEON\", top_n=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect ANAESTHETIST_TEAM\n",
    "No problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_column(data, \"ANAESTHETIST_TEAM\", top_n=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INSPECT ANAESTHETIST_MCR_NO\n",
    "No problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_column(data, \"ANAESTHETIST_MCR_NO\", top_n=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INSPECT ANESTHESIA\n",
    "No problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_column(data, \"ANESTHESIA\", top_n=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect EQUIPMENT\n",
    "Removed #NUH_ or #NUH from all entries, as well as alphabetically ordered the equipment such that even if they were in different orders, they would appear under the same unique value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_column(data, \"EQUIPMENT\", top_n=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synonyms = {}\n",
    "data = clean_equipment(\n",
    "    data,\n",
    "    col=\"EQUIPMENT\",\n",
    "    tags_to_strip=(r\"#nuh\",),        \n",
    "    unknown_vals=(\"0\",\"na\",\"n/a\",\"-\",\"null\",\"nan\",\"\"),\n",
    "    synonym_map=synonyms\n",
    ")\n",
    "\n",
    "# Inspect results\n",
    "inspect_column(data, \"EQUIPMENT\", top_n=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect ADMISSION_STATUS\n",
    "No problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_column(data, \"ADMISSION_STATUS\", top_n=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect ADMISSION_CLASS_TYPE\n",
    "No problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_column(data, \"ADMISSION_CLASS_TYPE\", top_n=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect ADMISSION_TYPE\n",
    "No problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_column(data, \"ADMISSION_TYPE\", top_n=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect ADMISSION_WARD\n",
    "No problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_column(data, \"ADMISSION_WARD\", top_n=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect ADMISSION_BED\n",
    "No problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_column(data, \"ADMISSION_BED\", top_n=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect AOH\n",
    "Fixed True False."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_column(data, \"AOH\", top_n=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = normalize_text(data, \"AOH\", unknown_vals=[\"0\", \"na\", \"n/a\", \"-\", \"null\", \"nan\"])\n",
    "inspect_column(data, \"AOH\", top_n=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect BLOOD\n",
    "No problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_column(data, \"BLOOD\", top_n=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect IMPLANT\n",
    "remove 'yes' remove 'x1' remove multiple white spaces, leading and trailing whitespaces and symbols."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_column(data, \"IMPLANT\", top_n=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = normalize_text(data, \"IMPLANT\", unknown_vals=[\"0\", \"na\", \"n/a\", \"-\", \"null\", \"nan\", \"\", \"nil\", \"nil.\"])\n",
    "data[\"IMPLANT\"] = (\n",
    "    data[\"IMPLANT\"]\n",
    "    .astype(str)\n",
    "    .str.strip(\" ;,.-\")\n",
    "    .str.replace(r\"\\bx\\d+\\b\", \"\", regex=True)\n",
    "    .str.replace(r\"\\byes\\b\", \"\", regex=True)\n",
    "    .str.replace(r\"\\s+\", \" \", regex=True)    \n",
    "    .str.strip()\n",
    "    .str.replace(\"&\", \"and\", regex=False)\n",
    ")\n",
    "inspect_column(data, \"IMPLANT\", top_n=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect DIAGNOSIS\n",
    "just normal standardisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_column(data, \"DIAGNOSIS\", top_n=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = normalize_text(data, \"DIAGNOSIS\", unknown_vals=[\"0\", \"na\", \"n/a\", \"-\", \"null\", \"nan\", \"\", \"nil\"])\n",
    "inspect_column(data, \"DIAGNOSIS\", top_n=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect CANCER_INDICATOR\n",
    "just normal standardisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_column(data, \"CANCER_INDICATOR\", top_n=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = normalize_text(data, \"CANCER_INDICATOR\", unknown_vals=[\"0\", \"na\", \"n/a\", \"-\", \"null\", \"nan\"])\n",
    "inspect_column(data, \"CANCER_INDICATOR\", top_n=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect TUMOR_INDICATOR\n",
    "just normal standardisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_column(data, \"TRAUMA_INDICATOR\", top_n=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = normalize_text(data, \"TRAUMA_INDICATOR\", unknown_vals=[\"0\", \"na\", \"n/a\", \"-\", \"null\", \"nan\"])\n",
    "inspect_column(data, \"TRAUMA_INDICATOR\", top_n=30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect Delay_Reason\n",
    "No problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_column(data, \"Delay_Reason\", top_n=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect Remarks\n",
    "No problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_column(data, \"Remarks\", top_n=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save file from cleaning steps above into a seperate file (change file name if required)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"oots-data-cleaning-1.xlsx\"\n",
    "data.to_excel(output_path, index=False)\n",
    "\n",
    "print(f\"Saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the highest frequency of words, bigrams, and trigrams to be used in taxonomy for categorisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_XLSX = \"oots-data-cleaning-1.xlsx\"\n",
    "\n",
    "DELAY_COL = \"Delay_Reason\"\n",
    "\n",
    "OUT_WORDS    = \"oots-delay-wordcount-cleaned-1.csv\"\n",
    "OUT_BIGRAMS  = \"oots-delay-bigram-count-cleaned-1.csv\"\n",
    "OUT_TRIGRAMS = \"oots-delay-trigram-count-cleaned-1.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(INPUT_XLSX)\n",
    "\n",
    "if DELAY_COL not in df.columns:\n",
    "    raise KeyError(f\"'{DELAY_COL}' not found. Available columns: {list(df.columns)}\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize text in Delay_Reason (remove punctuation, standardise case, remove trialing spaces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_punct_tbl = str.maketrans(\"\", \"\", string.punctuation)\n",
    "\n",
    "def normalize_text(s: str) -> str:\n",
    "\n",
    "    s = str(s).lower()\n",
    "    s = s.translate(_punct_tbl)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "\n",
    "    s = re.sub(r\"\\bo\\.t\\b\", \"operating theater\", s)\n",
    "    s = re.sub(r\"\\bot\\b\", \"operating theater\", s)\n",
    "    s = re.sub(r\"\\bo\\.r\\b\", \"operating room\", s)\n",
    "    s = re.sub(r\"\\banaesth\\b\", \"anaesthesia\", s)\n",
    "    s = re.sub(r\"\\banesth\\b\", \"anaesthesia\", s)\n",
    "    s = re.sub(r\"\\bpt\\b\", \"patient\", s)\n",
    "    s = re.sub(r\"\\bprev\\b\", \"previous\", s)\n",
    "    s = re.sub(r\"\\bdr\\b\", \"doctor\", s)\n",
    "    s = re.sub(r\"\\bpre-med\\b\", \"premedication\", s)\n",
    "\n",
    "    return s\n",
    "\n",
    "# Apply normalization\n",
    "df[\"_Delay_norm\"] = df[DELAY_COL].astype(str).fillna(\"\").map(normalize_text)\n",
    "df[[\"_Delay_norm\"]].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = {\n",
    "    \"the\",\"a\",\"an\",\"and\",\"or\",\"to\",\"of\",\"for\",\"by\",\"with\",\"from\",\n",
    "    \"is\",\"are\",\"was\",\"were\",\"be\",\"been\",\"being\",\"due\",\"because\",\n",
    "    \"this\",\"that\",\"it\",\"as\",\"into\",\"per\",\"via\", \"eg\", \"etc\"\n",
    "}\n",
    "\n",
    "# Initialize containers\n",
    "words, bigrams, trigrams = [], [], []\n",
    "\n",
    "# Tokenize each delay reason\n",
    "for text in df[\"_Delay_norm\"]:\n",
    "    tokens = [t for t in text.split() if t and t not in STOPWORDS]\n",
    "    if not tokens:\n",
    "        continue\n",
    "\n",
    "    words.extend(tokens)\n",
    "    if len(tokens) >= 2:\n",
    "        bigrams.extend([\" \".join(tokens[i:i+2]) for i in range(len(tokens)-1)])\n",
    "    if len(tokens) >= 3:\n",
    "        trigrams.extend([\" \".join(tokens[i:i+3]) for i in range(len(tokens)-2)])\n",
    "\n",
    "word_counts    = Counter(words)\n",
    "bigram_counts  = Counter(bigrams)\n",
    "trigram_counts = Counter(trigrams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(word_counts.most_common(),   columns=[\"Word\",   \"Count\"]).to_csv(OUT_WORDS,   index=False, encoding=\"utf-8-sig\")\n",
    "pd.DataFrame(bigram_counts.most_common(), columns=[\"Bigram\", \"Count\"]).to_csv(OUT_BIGRAMS, index=False, encoding=\"utf-8-sig\")\n",
    "pd.DataFrame(trigram_counts.most_common(),columns=[\"Trigram\",\"Count\"]).to_csv(OUT_TRIGRAMS,index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"Saved:\\n - {OUT_WORDS}\\n - {OUT_BIGRAMS}\\n - {OUT_TRIGRAMS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print top, words, bigrams, trigrams (change file according to updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n = 30\n",
    "\n",
    "print(\"\\nTop Words:\")\n",
    "display(pd.DataFrame(word_counts.most_common(top_n), columns=[\"Word\", \"Count\"]))\n",
    "\n",
    "print(\"\\nTop Bigrams:\")\n",
    "display(pd.DataFrame(bigram_counts.most_common(top_n), columns=[\"Bigram\", \"Count\"]))\n",
    "\n",
    "print(\"\\nTop Trigrams:\")\n",
    "display(pd.DataFrame(trigram_counts.most_common(top_n), columns=[\"Trigram\", \"Count\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "INPUT_FILE  = \"oots-data-cleaning-1.xlsx\"\n",
    "OUTPUT_FILE = \"oots-data-cleaning-3-flagged.xlsx\"\n",
    "OUTPUT_CSV  = \"oots-data-cleaning-3-flagged.csv\"\n",
    "\n",
    "df = pd.read_excel(INPUT_FILE)\n",
    "\n",
    "COL = \"Delay_Reason\"\n",
    "s = df[COL].astype(str)\n",
    "\n",
    "clean = (\n",
    "    s.str.lower()\n",
    "     .str.replace(r\"[^\\w\\s]\", \"\", regex=True)    \n",
    "     .str.replace(r\"\\s+\", \" \", regex=True)       \n",
    "     .str.strip()\n",
    ")\n",
    "\n",
    "raw = s.str.strip()\n",
    "only_punct_or_numbers = raw.str.match(r'^(?=.*\\S)(?!.*[A-Za-z]).*$', na=False)\n",
    "\n",
    "df.loc[only_punct_or_numbers, COL] = \"0\"\n",
    "\n",
    "not_late_phrases = [\n",
    "    \"no delay\", \"not delay\", \"not delayed\", \"not late\",\n",
    "    \"na\", \"0\", \"null\", \"nan\"\n",
    "]\n",
    "\n",
    "def phrase_to_token_pattern(p: str) -> str:\n",
    "    p = p.strip().lower()\n",
    "    esc = re.escape(p).replace(r\"\\ \", r\"\\s+\")\n",
    "    return rf\"(?<!\\w){esc}(?!\\w)\"\n",
    "\n",
    "pattern = r\"(?:{})\".format(\"|\".join(phrase_to_token_pattern(p) for p in not_late_phrases))\n",
    "regex = re.compile(pattern, flags=re.IGNORECASE)\n",
    "\n",
    "phrase_hit = clean.str.contains(regex, na=False)\n",
    "\n",
    "df[\"Reason_Is_Late\"] = np.where(only_punct_or_numbers | phrase_hit, 0, 1)\n",
    "\n",
    "df[COL] = clean\n",
    "df.loc[only_punct_or_numbers, COL] = \"0\"\n",
    "\n",
    "# Save\n",
    "df.to_excel(OUTPUT_FILE, index=False)\n",
    "df.to_csv(OUTPUT_CSV, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"Saved:\")\n",
    "print(f\" - {OUTPUT_FILE}\")\n",
    "print(f\" - {OUTPUT_CSV}\")\n",
    "print(df[[COL, \"Reason_Is_Late\"]].head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode Categorical Variables"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
