{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BT4103 Data Cleaning\n",
    "Please read through and let me know if there are any issues with regard to the cleaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages and datasets\n",
    "I have imported all the packages that I used up here for ease of reference. Please add your own filepath so that you can import the data correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import re\n",
    "import unicodedata\n",
    "import datetime\n",
    "import numpy as np\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing dataset\n",
    "Add filepath below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(\"oots-cleaned2.xlsx\")\n",
    "data\n",
    "\n",
    "validation_data = pd.read_excel(\"oots-cleaned-unlocked.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Helper Functions\n",
    "Here are the helper functions that I have created for easier readability in the actual code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OHE(df, col, drop_first=False): #One Hot Encode a column in a df\n",
    "    dummies = pd.get_dummies(df[col], prefix=col, drop_first=drop_first)\n",
    "    df = pd.concat([df.drop(columns=[col]), dummies], axis=1)\n",
    "    legend = {new_col: category \n",
    "              for new_col, category in zip(dummies.columns, dummies.columns.str.replace(f\"{col}_\", \"\", regex=False))}\n",
    "    return df, legend\n",
    "\n",
    "\n",
    "def LabelEncode(df, col): #Label Encode a column in a df\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col].astype(str))\n",
    "    legend = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "    return df, legend\n",
    "\n",
    "def inspect_column(df, col, top_n=20): #Get Unique Value information on column in df\n",
    "    print(f\"Column: {col}\")\n",
    "    print(f\"Unique values: {df[col].nunique(dropna=False)}\")\n",
    "    print(\"\\nTop value counts:\")\n",
    "    print(df[col].value_counts(dropna=False).head(top_n))\n",
    "\n",
    "def apply_mapping(df, col, mapping, new_col_suffix=\"_clean\"): #Apply new mapping to column in df\n",
    "    new_col = col + new_col_suffix\n",
    "    df[new_col] = df[col].replace(mapping)\n",
    "    return df\n",
    "\n",
    "def normalize_text(df, col, to_lower=True, replace_symbols=True, unknown_vals=None): #Normalise text of a column in a df\n",
    "    s = df[col].astype(str).str.strip()\n",
    "    if to_lower:\n",
    "        s = s.str.lower()\n",
    "    s = s.str.replace(r\"\\s+\", \" \", regex=True)\n",
    "    if replace_symbols:\n",
    "        s = s.str.replace(r\"[-_/]\", \" \", regex=True)\n",
    "    if unknown_vals:\n",
    "        s = s.replace(unknown_vals, \"0\")\n",
    "    df[col] = s\n",
    "    return df\n",
    "\n",
    "def _normalize_text_series(s: pd.Series) -> pd.Series: #Normalise text in series\n",
    "    s = s.astype(str).map(lambda x: unicodedata.normalize(\"NFKC\", x))\n",
    "    s = s.str.strip()\n",
    "    s = s.str.lower()\n",
    "    s = s.str.replace(r\"\\s+\", \" \", regex=True)         \n",
    "    s = s.str.replace(r\"\\s*/\\s*\", \" / \", regex=True)   \n",
    "    s = s.str.replace(r\"\\s*,\\s*\", \", \", regex=True)    \n",
    "    s = s.str.strip(\" ,\")                              \n",
    "    return s\n",
    "\n",
    "def _remove_trailing_code_in_parens(name_s: pd.Series, code_s: pd.Series) -> pd.Series: #remove white spaces in ()\n",
    "    code_up = code_s.astype(str).str.strip().str.upper()\n",
    "    pattern = r\"\\(\\s*{}\\s*\\)\\s*$\"\n",
    "    out = name_s.copy()\n",
    "    mask = code_up.notna() & code_up.ne(\"\")\n",
    "    out.loc[mask] = [\n",
    "        re.sub(pattern.format(re.escape(c)), \"\", n, flags=re.IGNORECASE)\n",
    "        for n, c in zip(out.loc[mask].tolist(), code_up.loc[mask].tolist())\n",
    "    ]\n",
    "    return out.str.strip(\" ,\")\n",
    "\n",
    "def _choose_canonical_name(name_series: pd.Series) -> str: #Chooses best name\n",
    "    s = name_series.dropna().astype(str)\n",
    "    s = s[s.str.strip().ne(\"\").values]\n",
    "    s = s[s.str.strip().ne(\"unknown\").values]\n",
    "    if s.empty:\n",
    "        return \"0\"\n",
    "    vc = s.value_counts()\n",
    "    top_freq = vc.iloc[0]\n",
    "    candidates = vc[vc.eq(top_freq)].index.tolist()\n",
    "    return max(candidates, key=len)\n",
    "\n",
    "def build_operation_legend_and_drop_nature( #Nature cleaning\n",
    "    df: pd.DataFrame,\n",
    "    code_col: str = \"OPERATION_CODE\",\n",
    "    nature_col: str = \"NATURE\",\n",
    "    drop_nature: bool = True,\n",
    "    keep_title_case_copy: bool = False,\n",
    "    unknown_tokens = (\"0\", \"na\", \"n/a\", \"-\", \"null\", \"nan\")\n",
    "):\n",
    "    if code_col not in df.columns or nature_col not in df.columns:\n",
    "        raise KeyError(f\"Expected columns '{code_col}' and '{nature_col}' in df.\")\n",
    "    work = pd.DataFrame({\n",
    "        \"operation_code\": df[code_col].astype(str).str.strip().str.upper(),\n",
    "        \"operation_name_raw\": df[nature_col]\n",
    "    })\n",
    "    name_norm = _normalize_text_series(work[\"operation_name_raw\"])\n",
    "    name_norm = name_norm.replace(list(unknown_tokens), \"unknown\")\n",
    "    name_clean = _remove_trailing_code_in_parens(name_norm, work[\"operation_code\"])\n",
    "    tmp = pd.DataFrame({\"operation_code\": work[\"operation_code\"], \"operation_name\": name_clean})\n",
    "    tmp = tmp[tmp[\"operation_code\"].str.len() > 0]\n",
    "    legend = (\n",
    "        tmp.groupby(\"operation_code\", as_index=False)[\"operation_name\"]\n",
    "           .apply(_choose_canonical_name)\n",
    "           .rename(columns={\"operation_name\": \"operation_name\"})\n",
    "    )\n",
    "    if keep_title_case_copy:\n",
    "        legend[\"operation_name_title\"] = legend[\"operation_name\"].str.title()\n",
    "    df_out = df.copy()\n",
    "    if drop_nature:\n",
    "        df_out.drop(columns=[nature_col], inplace=True, errors=\"ignore\")\n",
    "    return df_out, legend\n",
    "\n",
    "  \n",
    "def clean_equipment( #EQUIPMENT cleaning\n",
    "    df,\n",
    "    col=\"EQUIPMENT\",\n",
    "    sep=\";\",\n",
    "    tags_to_strip=(r\"#nuh\",),            \n",
    "    unknown_vals=(\"0\",\"na\",\"n/a\",\"-\",\"null\",\"nan\",\"\"),\n",
    "    synonym_map=None         \n",
    "):\n",
    "    if synonym_map is None:\n",
    "        synonym_map = {}\n",
    "    pattern = r\"|\".join(fr\"{re.escape(tag)}[_-]?\" for tag in tags_to_strip)\n",
    "    df[col] = (\n",
    "        df[col]\n",
    "        .astype(str)\n",
    "        .str.replace(pattern, \"\", regex=True, flags=re.IGNORECASE)\n",
    "    )\n",
    "    df = normalize_text(df, col, to_lower=True, replace_symbols=True, unknown_vals=unknown_vals)\n",
    "    def _clean_token(tok: str) -> str:\n",
    "        t = tok.strip()\n",
    "        if not t: return \"\"\n",
    "        t = re.sub(r\"\\s+\", \" \", t)\n",
    "        t = synonym_map.get(t, t)\n",
    "        if t in (\"unknown\",): return \"\"\n",
    "        return t\n",
    "    def _process_cell(cell: str) -> str:\n",
    "        parts = re.split(rf\"\\s*{re.escape(sep)}\\s*\", cell) if cell else []\n",
    "        cleaned = [_clean_token(p) for p in parts]\n",
    "        cleaned = [c for c in cleaned if c]\n",
    "        if not cleaned:\n",
    "            return \"unknown\"\n",
    "        cleaned = sorted(set(cleaned))\n",
    "        return f\"{sep} \".join(cleaned)\n",
    "    df[col] = df[col].apply(_process_cell)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Cleaning\n",
    "We are dropping index and case number as they are labels, and dropping patient name because it has been completely removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(data.columns[0], axis=1) # drop INDEX\n",
    "data = data.drop(columns=\"PATIENT_NAME\")\n",
    "data = data.drop(columns=\"CASE_NUMBER\")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensure correct data types\n",
    "yet to do, still waiting on update from joey regarding the dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle Missing Data\n",
    "Handling missing data is important to ensure that we do not run into any issues with the EDA, as well as our AI/ML implementations. It is a vital step in data cleaning to ensure that the dataset can be used efficiently and properly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling optional columns\n",
    "These columns potentially will be blank as there is nothing to write for some operations, hence we will fill those with 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Delay_Reason\"].fillna(0, inplace=True)\n",
    "data[\"Remarks\"].fillna(0, inplace=True)\n",
    "data[\"IMPLANT\"].fillna(0, inplace=True)\n",
    "data[\"EQUIPMENT\"].fillna(0, inplace=True)\n",
    "data[\"EMERGENCY_PRIORITY\"].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Correction (By Eyeballing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Date for row OPERATION_ID == 582117 should be 2010-04-19."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "planned_cols = [\n",
    "        \"PLANNED_PATIENT_CALL_TIME\",\n",
    "        \"PLANNED_PATIENT_FETCH_TIME\",\n",
    "        \"PLANNED_RECEPTION_IN_TIME\",\n",
    "        \"PLANNED_ENTER_OR_TIME\",\n",
    "        \"PLANNED_SURGERY_PREP_TIME\",\n",
    "        \"PLANNED_ANAESTHESIA_INDUCTION\",\n",
    "        \"PLANNED_KNIFE_TO_SKIN_TIME\",\n",
    "        \"PLANNED_SKIN_CLOSURE\",\n",
    "        \"PLANNED_PATIENT_REVERSAL_TIME\",\n",
    "        \"PLANNED_EXIT_OR_TIME\",\n",
    "        \"PLANNED_OR_CLEANUP_TIME\",\n",
    "        \"PLANNED_EXIT_RECOVERY_TIME\",        \n",
    "    ]\n",
    "\n",
    "actual_cols = [\n",
    "        \"PATIENT_CALL_TIME\",\n",
    "        \"PATIENT_FETCH_TIME\",\n",
    "        \"ACTUAL_RECEPTION_IN_TIME\",\n",
    "        \"ACTUAL_ENTER_OR_TIME\",\n",
    "        \"ACTUAL_SURGERY_PREP_TIME\",\n",
    "        \"ACTUAL_ANAESTHESIA_INDUCTION\",\n",
    "        \"ACTUAL_KNIFE_TO_SKIN_TIME\",\n",
    "        \"ACTUAL_SKIN_CLOSURE\",\n",
    "        \"ACTUAL_PATIENT_REVERSAL_TIME\",\n",
    "        \"ACTUAL_EXIT_OR_TIME\",\n",
    "        \"ACTUAL_OR_CLEANUP_TIME\",\n",
    "        \"ACTUAL_EXIT_RECOVERY_TIME\",        \n",
    "    ]\n",
    "\n",
    "# target row\n",
    "target_id = 582117\n",
    "fix_date = pd.Timestamp(\"2010-04-19\")\n",
    "\n",
    "mask = data[\"OPERATION_ID\"] == target_id\n",
    "\n",
    "def force_date(val, base_date):\n",
    "    if pd.isna(val):\n",
    "        return val\n",
    "    # If it's already a Timestamp\n",
    "    if isinstance(val, pd.Timestamp):\n",
    "        return pd.Timestamp.combine(base_date, val.time())\n",
    "    # If it's a datetime.time\n",
    "    if isinstance(val, datetime.time):\n",
    "        return pd.Timestamp.combine(base_date, val)\n",
    "    # Try to parse strings or other objects\n",
    "    try:\n",
    "        parsed = pd.to_datetime(val, errors=\"coerce\")\n",
    "        if pd.isna(parsed):\n",
    "            return val\n",
    "        return pd.Timestamp.combine(base_date, parsed.time())\n",
    "    except Exception:\n",
    "        return val\n",
    "\n",
    "for col in planned_cols:\n",
    "    if col in data.columns:\n",
    "        data.loc[mask, col] = data.loc[mask, col].apply(lambda v: force_date(v, fix_date))\n",
    "\n",
    "for col in actual_cols:\n",
    "    if col in data.columns:\n",
    "        data.loc[mask, col] = data.loc[mask, col].apply(lambda v: force_date(v, fix_date))        \n",
    "\n",
    "data.iloc[:, 7:31]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date Handling and Imputation Rules\n",
    "\n",
    "This section handles messy date/time data by enforcing **consistent start dates** and applying **domain-specific sync rules**.\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. Detecting Date vs. Time-only Strings\n",
    "- `_looks_like_date_string(s)` → checks if a string contains a date-like pattern (`YYYY-MM-DD`, `DD/MM/YYYY`, etc.).  \n",
    "- `_is_time_only_string(s)` → checks if a string looks like a time-only entry (`08:15`, `8:15:00 AM`, etc.).\n",
    "\n",
    "This distinction allows us to avoid misinterpreting time-only values as full datetimes.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Determining Constant Start Dates\n",
    "- `find_start_date_from_row(row, cols)` scans a list of columns in a row and finds the **first valid date**.  \n",
    "  - A valid date is a `Timestamp` with `year > 1900` or a parseable date string.  \n",
    "  - Returns the **normalized date** (time set to 00:00:00).  \n",
    "\n",
    "- `attach_constant_dates(row, planned_cols, actual_cols)`:\n",
    "  - Finds one **planned_start** and one **actual_start** per row.\n",
    "  - If `actual_start` is missing but `planned_start` exists, use the planned date as fallback.\n",
    "  - For any time-only strings, attach the corresponding start date to construct a full `Timestamp`.\n",
    "  - Full datetime values are preserved as-is.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2b. Date Sanitisation \n",
    "We discovered that some of the input data, while they had dates, had **corrupted or incorrectly manipulated dates**.  \n",
    "To ensure all downstream imputations are built on reliable timelines, we enforce a **sanity check**:\n",
    "\n",
    "1. **Valid date range**  \n",
    "   - Earliest allowed date: **2016-12-31**  \n",
    "   - Latest allowed date: **2022-02-25**\n",
    "\n",
    "2. **Correction procedure**  \n",
    "   - For each row, check if `planned_start` and `actual_start` fall within the valid range.  \n",
    "   - If either date is outside this range, attempt to backfill the correct value from  \n",
    "     `oots-cleaned-unlocked.xlsx` using `OPERATION_ID`.  \n",
    "   - If no match is found in the validation file, replace the invalid date with `NaT`.\n",
    "\n",
    "3. **Guarantees after cleaning**  \n",
    "   - Every `planned_start` and `actual_start` is either:\n",
    "     - Within the valid range, or  \n",
    "     - Backfilled from the validation dataset, or  \n",
    "     - Explicitly marked as `NaT` if no trusted source is available.  \n",
    "\n",
    "This step ensures that **all subsequent imputations** (e.g., filling missing times)  \n",
    "operate only on dates within the trusted window.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Row-wise Imputation Rules\n",
    "- `impute_with_rules(row, planned_cols, actual_cols)`:\n",
    "  1. **Attach constant start dates** using `attach_constant_dates`.\n",
    "  2. **Sync critical columns**:\n",
    "     - `PLANNED_PATIENT_CALL_TIME` ↔ `PLANNED_PATIENT_FETCH_TIME`  \n",
    "     - `PLANNED_OR_CLEANUP_TIME` ↔ `PLANNED_EXIT_OR_TIME`  \n",
    "     Preference is given to whichever value exists.\n",
    "  3. **Enforce ordering constraints**:\n",
    "     - Knife-to-skin ≤ Skin closure ≤ Patient reversal ≤ Exit OR ≤ Exit recovery ≤ OR cleanup\n",
    "     - If any step goes backwards, adjust forward.\n",
    "  4. **Fill missing anchor values**:\n",
    "     - If missing, `PLANNED_ANAESTHESIA_INDUCTION` and `PLANNED_SURGERY_PREP_TIME` are set to `PLANNED_KNIFE_TO_SKIN_TIME`.\n",
    "     - If missing, `PLANNED_PATIENT_REVERSAL_TIME` is set to `PLANNED_SKIN_CLOSURE`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_DATE = pd.Timestamp(\"2016-12-31\")\n",
    "MAX_DATE = pd.Timestamp(\"2022-02-25\")\n",
    "\n",
    "warnings = []\n",
    "\n",
    "_time_only_re = re.compile(r'^\\s*\\d{1,2}:\\d{2}(:\\d{2})?\\s*(?:[AaPp][Mm])?\\s*$')\n",
    "_date_like_re = re.compile(r'\\d{4}[-/]\\d{1,2}[-/]\\d{1,2}|\\d{1,2}[-/]\\d{1,2}[-/]\\d{2,4}')\n",
    "\n",
    "def _looks_like_date_string(s: str) -> bool:\n",
    "    \"\"\"Rudimentary check whether a string contains an explicit date.\"\"\"\n",
    "    if not isinstance(s, str):\n",
    "        return False\n",
    "    s = s.strip()\n",
    "    return bool(_date_like_re.search(s))\n",
    "\n",
    "def _is_time_only_string(s: str) -> bool:\n",
    "    \"\"\"True if string appears to contain time only (e.g. '08:15' or '8:15:00 AM').\"\"\"\n",
    "    if not isinstance(s, str):\n",
    "        return False\n",
    "    return bool(_time_only_re.match(s.strip()))\n",
    "\n",
    "def find_start_date_from_row(row, cols):\n",
    "    \"\"\"\n",
    "    Scan cols in order and return the first discovered 'date' (normalized).\n",
    "    We consider a value to contain a date if:\n",
    "      - it's a pandas Timestamp / datetime with a year > 1900\n",
    "      - or the original string contains a date-like pattern and parses to a Timestamp with sensible year\n",
    "    \"\"\"\n",
    "    for col in cols:\n",
    "        if col not in row.index:\n",
    "            continue\n",
    "        val = row[col]\n",
    "        if pd.isna(val):\n",
    "            continue\n",
    "\n",
    "        # Already a Timestamp / datetime\n",
    "        if isinstance(val, pd.Timestamp):\n",
    "            if val.year > 1900:  # treat as containing a real date\n",
    "                return val.normalize()\n",
    "            else:\n",
    "                # likely a parsed time-only; skip\n",
    "                continue\n",
    "        if isinstance(val, datetime.datetime):\n",
    "            if val.year > 1900:\n",
    "                return pd.Timestamp(val).normalize()\n",
    "\n",
    "        # If it's a string, check if it looks like a date first\n",
    "        try:\n",
    "            s = str(val).strip()\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "        if _looks_like_date_string(s):\n",
    "            parsed = pd.to_datetime(s, errors=\"coerce\", dayfirst=False)\n",
    "            if not pd.isna(parsed) and parsed.year > 1900:\n",
    "                return parsed.normalize()\n",
    "\n",
    "        # If it wasn't date-like, skip (likely time-only)\n",
    "    return None\n",
    "\n",
    "\n",
    "def attach_constant_dates(row, planned_cols, actual_cols):\n",
    "    \"\"\"\n",
    "    For a given row:\n",
    "     - find planned_start = first planned col that contains a date\n",
    "     - find actual_start  = first actual col that contains a date\n",
    "     - verify both are within MIN_DATE .. MAX_DATE; if not, try to retrieve from validation_date by OPERATION_ID\n",
    "     - for each planned col: if its value is time-only, attach planned_start\n",
    "     - for each actual col: if its value is time-only, attach actual_start\n",
    "    Does NOT override full datetimes.\n",
    "    \"\"\"\n",
    "    # Find constants (scan entire list until first real-date found)\n",
    "    planned_start = find_start_date_from_row(row, planned_cols)\n",
    "    actual_start = find_start_date_from_row(row, actual_cols)\n",
    "\n",
    "    # --- SANITY: if either start is missing or out of acceptable range, try to pull from validation sheet ---\n",
    "    def _in_range(ts):\n",
    "        return isinstance(ts, pd.Timestamp) and (MIN_DATE <= ts <= MAX_DATE)\n",
    "\n",
    "    # helper to attempt retrieval from validation_data using OPERATION_ID\n",
    "    def _try_validation_lookup(opid, cols):\n",
    "        if opid is None:\n",
    "            return None\n",
    "        # attempt direct match first; fall back to string match if needed\n",
    "        matches = pd.DataFrame()\n",
    "        try:\n",
    "            matches = validation_data.loc[validation_data[\"OPERATION_ID\"] == opid]\n",
    "        except Exception:\n",
    "            try:\n",
    "                matches = validation_data.loc[validation_data[\"OPERATION_ID\"].astype(str) == str(opid)]\n",
    "            except Exception:\n",
    "                matches = pd.DataFrame()\n",
    "\n",
    "        if matches is not None and not matches.empty:\n",
    "            val_row = matches.iloc[0]\n",
    "            return find_start_date_from_row(val_row, cols)\n",
    "        return None\n",
    "\n",
    "    opid = row.get(\"OPERATION_ID\", None)\n",
    "\n",
    "    # planned_start: if missing or out-of-range, attempt fallback from validation sheet\n",
    "    if not _in_range(planned_start):\n",
    "        alt_planned = _try_validation_lookup(opid, planned_cols)\n",
    "        if _in_range(alt_planned):\n",
    "            planned_start = alt_planned\n",
    "            warnings.append(f\"OPERATION_ID={opid}: planned_start replaced from validation file ({planned_start.date()}).\")\n",
    "        else:\n",
    "            warnings.append(f\"OPERATION_ID={opid}: planned_start {planned_start} out of range or missing; no valid replacement found in validation file.\")\n",
    "            planned_start = None\n",
    "\n",
    "    # actual_start: if missing or out-of-range, attempt fallback from validation sheet\n",
    "    if not _in_range(actual_start):\n",
    "        alt_actual = _try_validation_lookup(opid, actual_cols)\n",
    "        if _in_range(alt_actual):\n",
    "            actual_start = alt_actual\n",
    "            warnings.append(f\"OPERATION_ID={opid}: actual_start replaced from validation file ({actual_start.date()}).\")\n",
    "        else:\n",
    "            # if no actual start in validation, we will keep None for now and allow later fallback to planned_start\n",
    "            warnings.append(f\"OPERATION_ID={opid}: actual_start {actual_start} out of range or missing; no valid replacement found in validation file.\")\n",
    "            actual_start = None\n",
    "\n",
    "    # --- NEW: fallback (if actual still missing, use planned_start) ---\n",
    "    if actual_start is None and planned_start is not None:\n",
    "        actual_start = planned_start\n",
    "\n",
    "    # Helper to combine time-only string with a start date\n",
    "    def _combine_time_with_date(s, base_date):\n",
    "        # parse the time string into a datetime (may get today's date, we only use .time())\n",
    "        parsed = pd.to_datetime(s, errors=\"coerce\")\n",
    "        if pd.isna(parsed) or base_date is None:\n",
    "            return None\n",
    "        return pd.Timestamp.combine(base_date, parsed.time())\n",
    "\n",
    "    # Fill planned cols\n",
    "    for col in planned_cols:\n",
    "        if col not in row.index:\n",
    "            continue\n",
    "        val = row[col]\n",
    "        if pd.isna(val):\n",
    "            continue\n",
    "\n",
    "        # if it's already a Timestamp or datetime\n",
    "        if isinstance(val, (pd.Timestamp, datetime.datetime)):\n",
    "            ts = pd.Timestamp(val)\n",
    "            if MIN_DATE <= ts <= MAX_DATE:\n",
    "                row[col] = ts\n",
    "                continue\n",
    "            else:\n",
    "                # invalid → try to replace with same time on planned_start\n",
    "                if planned_start is not None:\n",
    "                    row[col] = pd.Timestamp.combine(planned_start, ts.time())\n",
    "                else:\n",
    "                    row[col] = pd.NaT\n",
    "            continue\n",
    "\n",
    "        # otherwise, treat it as string\n",
    "        s = str(val).strip()\n",
    "        if _is_time_only_string(s):\n",
    "            if planned_start is not None:\n",
    "                combined = _combine_time_with_date(s, planned_start)\n",
    "                if combined is not None:\n",
    "                    row[col] = combined\n",
    "        else:\n",
    "            parsed = pd.to_datetime(s, errors=\"coerce\", dayfirst=False)\n",
    "            if not pd.isna(parsed):\n",
    "                if MIN_DATE <= parsed <= MAX_DATE:\n",
    "                    row[col] = parsed\n",
    "                elif planned_start is not None:\n",
    "                    row[col] = pd.Timestamp.combine(planned_start, parsed.time())\n",
    "                else:\n",
    "                    row[col] = pd.NaT\n",
    "\n",
    "    # Fill actual cols (same logic)\n",
    "    for col in actual_cols:\n",
    "        if col not in row.index:\n",
    "            continue\n",
    "        val = row[col]\n",
    "        if pd.isna(val):\n",
    "            continue\n",
    "\n",
    "        if isinstance(val, (pd.Timestamp, datetime.datetime)):\n",
    "            ts = pd.Timestamp(val)\n",
    "            if MIN_DATE <= ts <= MAX_DATE:\n",
    "                row[col] = ts\n",
    "                continue\n",
    "            else:\n",
    "                if actual_start is not None:\n",
    "                    row[col] = pd.Timestamp.combine(actual_start, ts.time())\n",
    "                else:\n",
    "                    row[col] = pd.NaT\n",
    "            continue\n",
    "\n",
    "        s = str(val).strip()\n",
    "        if _is_time_only_string(s):\n",
    "            if actual_start is not None:\n",
    "                combined = _combine_time_with_date(s, actual_start)\n",
    "                if combined is not None:\n",
    "                    row[col] = combined\n",
    "        else:\n",
    "            parsed = pd.to_datetime(s, errors=\"coerce\", dayfirst=False)\n",
    "            if not pd.isna(parsed):\n",
    "                if MIN_DATE <= parsed <= MAX_DATE:\n",
    "                    row[col] = parsed\n",
    "                elif actual_start is not None:\n",
    "                    row[col] = pd.Timestamp.combine(actual_start, parsed.time())\n",
    "                else:\n",
    "                    row[col] = pd.NaT\n",
    "\n",
    "\n",
    "    return row\n",
    "\n",
    "\n",
    "def impute_with_rules(row, planned_cols, actual_cols):\n",
    "    \"\"\"\n",
    "    Step 1: Attach constant planned/actual start dates\n",
    "    Step 2: Apply sync rules (with bias toward existing non-null values)\n",
    "    Step 3: Fill missing anaesthesia/prep times from knife-to-skin\n",
    "    \"\"\"\n",
    "    # --- Step 1: attach constant dates ---\n",
    "    row = attach_constant_dates(row, planned_cols, actual_cols)\n",
    "\n",
    "    # --- Step 2: enforce logical sync rules ---\n",
    "    def sync_cols(col_a, col_b, prefer=\"a\"):\n",
    "        \"\"\"Sync two columns with preference if one is missing.\"\"\"\n",
    "        a, b = row.get(col_a, pd.NaT), row.get(col_b, pd.NaT)\n",
    "        if pd.isna(a) and pd.notna(b):\n",
    "            row[col_a] = b\n",
    "        elif pd.isna(b) and pd.notna(a):\n",
    "            row[col_b] = a\n",
    "        elif pd.notna(a) and pd.notna(b):\n",
    "            if prefer == \"a\":\n",
    "                row[col_b] = a\n",
    "            else:\n",
    "                row[col_a] = b\n",
    "\n",
    "    # Rule 1: PLANNED_PATIENT_CALL_TIME == PLANNED_PATIENT_FETCH_TIME\n",
    "    sync_cols(\"PLANNED_PATIENT_CALL_TIME\", \"PLANNED_PATIENT_FETCH_TIME\", prefer=\"fetch\")\n",
    "\n",
    "    # Rule 2: PLANNED_OR_CLEANUP_TIME == PLANNED_EXIT_OR_TIME\n",
    "    sync_cols(\"PLANNED_OR_CLEANUP_TIME\", \"PLANNED_EXIT_OR_TIME\", prefer=\"exit\")\n",
    "\n",
    "    # Rule 3: Ensure ordering constraints (only if both present)\n",
    "    def enforce_order(before, after):\n",
    "        if before in row.index and after in row.index:\n",
    "            if pd.notna(row[before]) and pd.notna(row[after]):\n",
    "                try:\n",
    "                    if row[after] < row[before]:\n",
    "                        row[after] = row[before]\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "    enforce_order(\"PLANNED_KNIFE_TO_SKIN_TIME\", \"PLANNED_SKIN_CLOSURE\")\n",
    "    enforce_order(\"PLANNED_SKIN_CLOSURE\", \"PLANNED_PATIENT_REVERSAL_TIME\")\n",
    "    enforce_order(\"PLANNED_PATIENT_REVERSAL_TIME\", \"PLANNED_EXIT_OR_TIME\")\n",
    "    enforce_order(\"PLANNED_EXIT_OR_TIME\", \"PLANNED_EXIT_RECOVERY_TIME\")\n",
    "    enforce_order(\"PLANNED_EXIT_RECOVERY_TIME\", \"PLANNED_OR_CLEANUP_TIME\")\n",
    "\n",
    "    # --- Step 3: fill missing times from anchors ---\n",
    "    knife = row.get(\"PLANNED_KNIFE_TO_SKIN_TIME\", pd.NaT)\n",
    "    closure = row.get(\"PLANNED_SKIN_CLOSURE\", pd.NaT)\n",
    "\n",
    "    if pd.notna(knife):\n",
    "        if \"PLANNED_ANAESTHESIA_INDUCTION\" in row.index and pd.isna(row[\"PLANNED_ANAESTHESIA_INDUCTION\"]):\n",
    "            row[\"PLANNED_ANAESTHESIA_INDUCTION\"] = knife\n",
    "        if \"PLANNED_SURGERY_PREP_TIME\" in row.index and pd.isna(row[\"PLANNED_SURGERY_PREP_TIME\"]):\n",
    "            row[\"PLANNED_SURGERY_PREP_TIME\"] = knife\n",
    "\n",
    "    if pd.notna(closure):\n",
    "        if \"PLANNED_PATIENT_REVERSAL_TIME\" in row.index and pd.isna(row[\"PLANNED_PATIENT_REVERSAL_TIME\"]):\n",
    "            row[\"PLANNED_PATIENT_REVERSAL_TIME\"] = closure\n",
    "\n",
    "    return row\n",
    "\n",
    "# Apply row-wise\n",
    "data = data.apply(lambda r: impute_with_rules(r, planned_cols, actual_cols), axis=1)\n",
    "data.iloc[:, 7:31]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Date processing for ACTUAL columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_patient_times(row):\n",
    "    fetch_col = \"PATIENT_FETCH_TIME\"\n",
    "    call_col = \"PATIENT_CALL_TIME\"\n",
    "    reception_col = \"ACTUAL_RECEPTION_IN_TIME\"\n",
    "\n",
    "    # Helper: coerce any value to Timestamp\n",
    "    def _to_ts(val):\n",
    "        if pd.isna(val):\n",
    "            return None\n",
    "        if isinstance(val, pd.Timestamp):\n",
    "            return val\n",
    "        try:\n",
    "            return pd.to_datetime(val, errors=\"coerce\")\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    # --- Step 1: if CALL is empty, copy RECEPTION ---\n",
    "    call_time = _to_ts(row.get(call_col, pd.NaT))\n",
    "    reception_time = _to_ts(row.get(reception_col, pd.NaT))\n",
    "\n",
    "    if call_time is None and reception_time is not None:\n",
    "        row[call_col] = reception_time\n",
    "        call_time = reception_time\n",
    "\n",
    "    # --- Step 2: if FETCH is empty, fill it ---\n",
    "    fetch_time = _to_ts(row.get(fetch_col, pd.NaT))\n",
    "    if fetch_time is None:\n",
    "        if call_time is not None and reception_time is not None:\n",
    "            midpoint = call_time + (reception_time - call_time) / 2\n",
    "            row[fetch_col] = midpoint.floor(\"min\")  # round down\n",
    "        elif call_time is not None:\n",
    "            row[fetch_col] = call_time.floor(\"min\")\n",
    "        elif reception_time is not None:\n",
    "            row[fetch_col] = reception_time.floor(\"min\")\n",
    "\n",
    "    return row\n",
    "\n",
    "data = data.apply(impute_patient_times, axis=1)\n",
    "\n",
    "data.iloc[:, 19:31]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputing Missing Induction, Prep, and Reversal Times\n",
    "\n",
    "For some rows, `ACTUAL_ANAESTHESIA_INDUCTION`, `ACTUAL_SURGERY_PREP_TIME`,   \n",
    "`ACTUAL_PATIENT_REVERSAL_TIME`, and `ACTUAL_OR_CLEANUP_TIME` are missing.  \n",
    "To fill these values in a consistent and data-driven way, we treat the **OR workflow as a timeline**.\n",
    "\n",
    "---\n",
    "\n",
    "#### Case A: Induction & Prep together (baseline method)\n",
    "- `ACTUAL_ENTER_OR_TIME` → **0% mark**  \n",
    "- `ACTUAL_KNIFE_TO_SKIN_TIME` → **100% mark**  \n",
    "\n",
    "For rows where both induction and prep times are available, we compute their relative positions:\n",
    "- **Induction mark** = (Induction − Enter OR) ÷ (Knife-to-skin − Enter OR)  \n",
    "- **Prep mark** = (Prep − Enter OR) ÷ (Knife-to-skin − Enter OR)  \n",
    "\n",
    "We then take the **average mark** across all valid rows.  \n",
    "For rows with missing values:\n",
    "- `ACTUAL_ANAESTHESIA_INDUCTION` is backfilled as  \n",
    "  `Enter OR + (Knife-to-skin − Enter OR) × <avg induction mark>`, rounded to the nearest minute  \n",
    "- `ACTUAL_SURGERY_PREP_TIME` is backfilled as  \n",
    "  `Enter OR + (Knife-to-skin − Enter OR) × <avg prep mark>`, rounded to the nearest minute  \n",
    "\n",
    "---\n",
    "\n",
    "#### Case B: Prep missing, but induction & knife available\n",
    "For rows with induction and knife-to-skin times but missing prep:  \n",
    "- Compute average **prep-from-induction mark** = (Prep − Induction) ÷ (Knife − Induction)  \n",
    "- Backfill missing prep as  \n",
    "  `Induction + (Knife − Induction) × <avg prep-from-induction mark>`\n",
    "\n",
    "---\n",
    "\n",
    "#### Case C: Induction missing, but enter & prep available\n",
    "For rows with enter and prep but missing induction:  \n",
    "- Compute average **induction-from-enter mark** = (Induction − Enter OR) ÷ (Prep − Enter OR)  \n",
    "- Backfill missing induction as  \n",
    "  `Enter OR + (Prep − Enter OR) × <avg induction-from-enter mark>`\n",
    "\n",
    "---\n",
    "\n",
    "#### Case D: Reversal missing, but closure & exit available\n",
    "For rows with closure and exit but missing reversal:  \n",
    "- `ACTUAL_SKIN_CLOSURE` → **0% mark**  \n",
    "- `ACTUAL_EXIT_OR_TIME` → **100% mark**  \n",
    "- Compute average **reversal mark** = (Reversal − Closure) ÷ (Exit − Closure)  \n",
    "- Backfill missing reversal as  \n",
    "  `Closure + (Exit − Closure) × <avg reversal mark>`, rounded to the nearest minute  \n",
    "\n",
    "---\n",
    "\n",
    "#### Case E: Cleanup missing, but exit available\n",
    "Unlike induction, prep, and reversal, cleanup is best modeled as a **fixed offset** after exit.  \n",
    "- Compute average **cleanup offset** = (Cleanup − Exit) across rows with both values.  \n",
    "- Backfill missing cleanup as  \n",
    "  `Exit + <avg cleanup offset>`  \n",
    "Rounded to the nearest minute.\n",
    "\n",
    "---\n",
    "\n",
    "This imputation strategy ensures that filled values preserve the natural ordering of OR events, are grounded in real observed distributions, and remain realistic within the surgical timeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_marks(data):\n",
    "    marks = {}\n",
    "\n",
    "    # Case A: induction & prep relative to enter/knife\n",
    "    mask = (\n",
    "        data[\"ACTUAL_ENTER_OR_TIME\"].notna()\n",
    "        & data[\"ACTUAL_ANAESTHESIA_INDUCTION\"].notna()\n",
    "        & data[\"ACTUAL_SURGERY_PREP_TIME\"].notna()\n",
    "        & data[\"ACTUAL_KNIFE_TO_SKIN_TIME\"].notna()\n",
    "    )\n",
    "    clean = data.loc[mask].copy()\n",
    "    clean = clean[\n",
    "        (clean[\"ACTUAL_ENTER_OR_TIME\"] <= clean[\"ACTUAL_ANAESTHESIA_INDUCTION\"])\n",
    "        & (clean[\"ACTUAL_ANAESTHESIA_INDUCTION\"] <= clean[\"ACTUAL_SURGERY_PREP_TIME\"])\n",
    "        & (clean[\"ACTUAL_SURGERY_PREP_TIME\"] <= clean[\"ACTUAL_KNIFE_TO_SKIN_TIME\"])\n",
    "    ]\n",
    "    if not clean.empty:\n",
    "        total = (clean[\"ACTUAL_KNIFE_TO_SKIN_TIME\"] - clean[\"ACTUAL_ENTER_OR_TIME\"]).dt.total_seconds()\n",
    "        marks[\"induction\"] = ((clean[\"ACTUAL_ANAESTHESIA_INDUCTION\"] - clean[\"ACTUAL_ENTER_OR_TIME\"]).dt.total_seconds() / total).mean(skipna=True)\n",
    "        marks[\"prep\"] = ((clean[\"ACTUAL_SURGERY_PREP_TIME\"] - clean[\"ACTUAL_ENTER_OR_TIME\"]).dt.total_seconds() / total).mean(skipna=True)\n",
    "\n",
    "    # Case B: prep relative to induction/knife\n",
    "    mask = (\n",
    "        data[\"ACTUAL_ANAESTHESIA_INDUCTION\"].notna()\n",
    "        & data[\"ACTUAL_SURGERY_PREP_TIME\"].notna()\n",
    "        & data[\"ACTUAL_KNIFE_TO_SKIN_TIME\"].notna()\n",
    "    )\n",
    "    clean = data.loc[mask].copy()\n",
    "    clean = clean[\n",
    "        (clean[\"ACTUAL_ANAESTHESIA_INDUCTION\"] <= clean[\"ACTUAL_SURGERY_PREP_TIME\"])\n",
    "        & (clean[\"ACTUAL_SURGERY_PREP_TIME\"] <= clean[\"ACTUAL_KNIFE_TO_SKIN_TIME\"])\n",
    "    ]\n",
    "    if not clean.empty:\n",
    "        total = (clean[\"ACTUAL_KNIFE_TO_SKIN_TIME\"] - clean[\"ACTUAL_ANAESTHESIA_INDUCTION\"]).dt.total_seconds()\n",
    "        marks[\"prep_from_induction\"] = ((clean[\"ACTUAL_SURGERY_PREP_TIME\"] - clean[\"ACTUAL_ANAESTHESIA_INDUCTION\"]).dt.total_seconds() / total).mean(skipna=True)\n",
    "\n",
    "    # Case C: induction relative to enter/prep\n",
    "    mask = (\n",
    "        data[\"ACTUAL_ENTER_OR_TIME\"].notna()\n",
    "        & data[\"ACTUAL_ANAESTHESIA_INDUCTION\"].notna()\n",
    "        & data[\"ACTUAL_SURGERY_PREP_TIME\"].notna()\n",
    "    )\n",
    "    clean = data.loc[mask].copy()\n",
    "    clean = clean[\n",
    "        (clean[\"ACTUAL_ENTER_OR_TIME\"] <= clean[\"ACTUAL_ANAESTHESIA_INDUCTION\"])\n",
    "        & (clean[\"ACTUAL_ANAESTHESIA_INDUCTION\"] <= clean[\"ACTUAL_SURGERY_PREP_TIME\"])\n",
    "    ]\n",
    "    if not clean.empty:\n",
    "        total = (clean[\"ACTUAL_SURGERY_PREP_TIME\"] - clean[\"ACTUAL_ENTER_OR_TIME\"]).dt.total_seconds()\n",
    "        marks[\"induction_from_enter\"] = ((clean[\"ACTUAL_ANAESTHESIA_INDUCTION\"] - clean[\"ACTUAL_ENTER_OR_TIME\"]).dt.total_seconds() / total).mean(skipna=True)\n",
    "\n",
    "    # Case D: reversal relative to closure/exit\n",
    "    mask = (\n",
    "        data[\"ACTUAL_SKIN_CLOSURE\"].notna()\n",
    "        & data[\"ACTUAL_PATIENT_REVERSAL_TIME\"].notna()\n",
    "        & data[\"ACTUAL_EXIT_OR_TIME\"].notna()\n",
    "    )\n",
    "    clean = data.loc[mask].copy()\n",
    "    clean = clean[\n",
    "        (clean[\"ACTUAL_SKIN_CLOSURE\"] <= clean[\"ACTUAL_PATIENT_REVERSAL_TIME\"])\n",
    "        & (clean[\"ACTUAL_PATIENT_REVERSAL_TIME\"] <= clean[\"ACTUAL_EXIT_OR_TIME\"])\n",
    "    ]\n",
    "    if not clean.empty:\n",
    "        total = (clean[\"ACTUAL_EXIT_OR_TIME\"] - clean[\"ACTUAL_SKIN_CLOSURE\"]).dt.total_seconds()\n",
    "        marks[\"reversal\"] = ((clean[\"ACTUAL_PATIENT_REVERSAL_TIME\"] - clean[\"ACTUAL_SKIN_CLOSURE\"]).dt.total_seconds() / total).mean(skipna=True)\n",
    "\n",
    "    # Case E: cleanup offset from exit\n",
    "    mask = (\n",
    "        data[\"ACTUAL_EXIT_OR_TIME\"].notna()\n",
    "        & data[\"ACTUAL_OR_CLEANUP_TIME\"].notna()\n",
    "    )\n",
    "    clean = data.loc[mask].copy()\n",
    "\n",
    "    # Keep only realistic differences (0 to 12 hours after exit)\n",
    "    valid = (clean[\"ACTUAL_OR_CLEANUP_TIME\"] >= clean[\"ACTUAL_EXIT_OR_TIME\"]) & (\n",
    "        (clean[\"ACTUAL_OR_CLEANUP_TIME\"] - clean[\"ACTUAL_EXIT_OR_TIME\"]) <= pd.Timedelta(hours=12)\n",
    "    )\n",
    "    clean = clean[valid]\n",
    "\n",
    "    if not clean.empty:\n",
    "        diffs = (clean[\"ACTUAL_OR_CLEANUP_TIME\"] - clean[\"ACTUAL_EXIT_OR_TIME\"]).dt.total_seconds()\n",
    "        marks[\"cleanup_offset\"] = round(diffs.mean(skipna=True) / 60.0)  # minutes\n",
    "\n",
    "\n",
    "    return marks\n",
    "\n",
    "\n",
    "def impute_induction_prep_reversal_cleanup(row, marks):\n",
    "    enter, induction, prep, knife = row[[\"ACTUAL_ENTER_OR_TIME\", \"ACTUAL_ANAESTHESIA_INDUCTION\", \"ACTUAL_SURGERY_PREP_TIME\", \"ACTUAL_KNIFE_TO_SKIN_TIME\"]]\n",
    "    closure, reversal, exit_, cleanup = row[[\"ACTUAL_SKIN_CLOSURE\", \"ACTUAL_PATIENT_REVERSAL_TIME\", \"ACTUAL_EXIT_OR_TIME\", \"ACTUAL_OR_CLEANUP_TIME\"]]\n",
    "\n",
    "    # --- Case A: both missing induction & prep\n",
    "    if pd.notna(enter) and pd.isna(induction) and pd.isna(prep) and pd.notna(knife):\n",
    "        if \"induction\" in marks and \"prep\" in marks:\n",
    "            total = knife - enter\n",
    "            row[\"ACTUAL_ANAESTHESIA_INDUCTION\"] = (enter + total * marks[\"induction\"]).round(\"min\")\n",
    "            row[\"ACTUAL_SURGERY_PREP_TIME\"] = (enter + total * marks[\"prep\"]).round(\"min\")\n",
    "\n",
    "    # --- Case B: missing prep only\n",
    "    if pd.notna(induction) and pd.isna(prep) and pd.notna(knife):\n",
    "        if \"prep_from_induction\" in marks:\n",
    "            total = knife - induction\n",
    "            row[\"ACTUAL_SURGERY_PREP_TIME\"] = (induction + total * marks[\"prep_from_induction\"]).round(\"min\")\n",
    "\n",
    "    # --- Case C: missing induction only\n",
    "    if pd.notna(enter) and pd.isna(induction) and pd.notna(prep):\n",
    "        if \"induction_from_enter\" in marks:\n",
    "            total = prep - enter\n",
    "            row[\"ACTUAL_ANAESTHESIA_INDUCTION\"] = (enter + total * marks[\"induction_from_enter\"]).round(\"min\")\n",
    "\n",
    "    # --- Case D: missing reversal\n",
    "    if pd.notna(closure) and pd.isna(reversal) and pd.notna(exit_):\n",
    "        if \"reversal\" in marks:\n",
    "            total = exit_ - closure\n",
    "            row[\"ACTUAL_PATIENT_REVERSAL_TIME\"] = (closure + total * marks[\"reversal\"]).round(\"min\")\n",
    "\n",
    "    # --- Case E: missing cleanup\n",
    "    if pd.notna(exit_) and pd.isna(cleanup):\n",
    "        if \"cleanup_offset\" in marks:\n",
    "            row[\"ACTUAL_OR_CLEANUP_TIME\"] = (exit_ + pd.Timedelta(minutes=marks[\"cleanup_offset\"])).round(\"min\")\n",
    "\n",
    "    return row\n",
    "\n",
    "\n",
    "# Step 1: get average marks\n",
    "marks = compute_marks(data)\n",
    "\n",
    "# Step 2: apply backfill\n",
    "data = data.apply(lambda r: impute_induction_prep_reversal_cleanup(r, marks), axis=1)\n",
    "\n",
    "data.iloc[:, 19:31]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle Rows with no Actual Data\n",
    "\n",
    "For the purpose of our project, actual date/time data is needed to track any delays with the planned time.\n",
    "By observation, we note that these rows with no actual data track to procedures marked with LOCATION == \"OUT OF OT ROOMS\".\n",
    "These represent cases outside of operating theatres and should not be included in downstream time sequence analysis. We therefore remove them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "before = len(data)\n",
    "data = data[data[\"LOCATION\"] != \"OUT OF OT ROOMS\"].copy()\n",
    "after = len(data)\n",
    "\n",
    "print(f\"Removed {before - after} rows with LOCATION == 'OUT OF OT ROOMS' (kept {after}).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Planned Columns to Datetime\n",
    "\n",
    "To make sure all planned time columns are in a consistent `datetime64[ns]` format, we explicitly convert them using `pd.to_datetime`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all planned columns to datetime64[ns]\n",
    "for col in planned_cols:\n",
    "    if col in data.columns:\n",
    "        data[col] = pd.to_datetime(data[col], errors=\"coerce\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle admission related columns\n",
    "Some of these surgeries may be day surgeries of from the A&E, hence might not have admission data. Hence, we will replace blanks with \"Not Admitted\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "admission_cols = [\"ADMISSION_STATUS\", \"ADMISSION_CLASS_TYPE\", \n",
    "                  \"ADMISSION_TYPE\", \"ADMISSION_WARD\", \"ADMISSION_BED\"]\n",
    "data[admission_cols] = data[admission_cols].fillna(\"Not Admitted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill in missing staff data\n",
    "Some surgeries are missing surgeon, anaesthetist, or diagnosis data, hence we will fill it with \"Unknown\" and \"Not Recorded\". This is because it is likely not possible for a surgery to proceed without them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clinician_cols = [\"SURGEON\", \"ANAESTHETIST_TEAM\", \"ANAESTHETIST_MCR_NO\"]\n",
    "data[clinician_cols] = data[clinician_cols].fillna(\"Unknown\")\n",
    "data[\"DIAGNOSIS\"] = data[\"DIAGNOSIS\"].fillna(\"Not Recorded\")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop remaining missing rows\n",
    "After filling in the missing values that we are able to fill, there are some columns that are still missing data. We will thus drop them as they make up a very small portion of our overall data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View current state of dataframe\n",
    "Currently, the dataset no longer contains any missing data, and thus we are able to proceed with the next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle Duplicate Data\n",
    "Important to remove to prevent bias in our AI/ML solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for duplicate rows\n",
    "This is to see if our dataset contains any rows that are completely identical. This means that the same surgery has been accidentally logged twice. We want to avoid having this in our dataset as it would cause our analysis in the future to skew."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop duplicate rows\n",
    "We identified 3 duplicate rows, and hence we will want to drop them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep cleaning each column\n",
    "Looking into each individual column to clean up most of the free text portions. Please add more cleaning as we go, as there is quite alot to sieve through and I dont think i caught it all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect Location\n",
    "No problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unique values analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_column(data, \"LOCATION\", top_n=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect Room\n",
    "No problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unique values analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_column(data, \"ROOM\", top_n=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect case status\n",
    "No problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unique values analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_column(data, \"CASE_STATUS\", top_n=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect OPERATION_TYPE\n",
    "No problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_column(data, \"OPERATION_TYPE\", top_n=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect Emergency Priority\n",
    "No problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_column(data, \"EMERGENCY_PRIORITY\", top_n=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect Patient Code\n",
    "No problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_column(data, \"PATIENT_CODE\", top_n=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect Nature\n",
    "Removed this column entirely, and created a legend(can be found below) to map SURGICAL_CODE to NATURE, as they are the same thing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_column(data, \"NATURE\", top_n=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, nature_legend = build_operation_legend_and_drop_nature(\n",
    "    data,\n",
    "    code_col=\"SURGICAL_CODE\",\n",
    "    nature_col=\"NATURE\",\n",
    "    drop_nature=True,           \n",
    "    keep_title_case_copy=True    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.columns)              \n",
    "nature_legend.drop(columns='operation_name_title', inplace=True)\n",
    "nature_legend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect Surgical Code\n",
    "Extension of NATURE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_column(data, \"SURGICAL_CODE\", top_n=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect discipline\n",
    "No problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_column(data, \"DISCIPLINE\", top_n=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect Surgeon\n",
    "No problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_column(data, \"SURGEON\", top_n=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect ANAESTHETIST_TEAM\n",
    "No problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_column(data, \"ANAESTHETIST_TEAM\", top_n=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INSPECT ANAESTHETIST_MCR_NO\n",
    "No problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_column(data, \"ANAESTHETIST_MCR_NO\", top_n=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INSPECT ANESTHESIA\n",
    "No problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_column(data, \"ANESTHESIA\", top_n=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect EQUIPMENT\n",
    "Removed #NUH_ or #NUH from all entries, as well as alphabetically ordered the equipment such that even if they were in different orders, they would appear under the same unique value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_column(data, \"EQUIPMENT\", top_n=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synonyms = {}\n",
    "data = clean_equipment(\n",
    "    data,\n",
    "    col=\"EQUIPMENT\",\n",
    "    tags_to_strip=(r\"#nuh\",),        \n",
    "    unknown_vals=(\"0\",\"na\",\"n/a\",\"-\",\"null\",\"nan\",\"\"),\n",
    "    synonym_map=synonyms\n",
    ")\n",
    "\n",
    "# Inspect results\n",
    "inspect_column(data, \"EQUIPMENT\", top_n=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect ADMISSION_STATUS\n",
    "No problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_column(data, \"ADMISSION_STATUS\", top_n=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data[\"ADMISSION_STATUS\"] != \"1518656227\"]\n",
    "data = data[data[\"ADMISSION_STATUS\"] != \"1518637975\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect ADMISSION_CLASS_TYPE\n",
    "No problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_column(data, \"ADMISSION_CLASS_TYPE\", top_n=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect ADMISSION_TYPE\n",
    "No problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_column(data, \"ADMISSION_TYPE\", top_n=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect ADMISSION_WARD\n",
    "No problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_column(data, \"ADMISSION_WARD\", top_n=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect ADMISSION_BED\n",
    "No problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_column(data, \"ADMISSION_BED\", top_n=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect AOH\n",
    "Fixed True False."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_column(data, \"AOH\", top_n=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = normalize_text(data, \"AOH\", unknown_vals=[\"0\", \"na\", \"n/a\", \"-\", \"null\", \"nan\"])\n",
    "inspect_column(data, \"AOH\", top_n=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect BLOOD\n",
    "No problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_column(data, \"BLOOD\", top_n=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect IMPLANT\n",
    "remove 'yes' remove 'x1' remove multiple white spaces, leading and trailing whitespaces and symbols."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_column(data, \"IMPLANT\", top_n=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = normalize_text(data, \"IMPLANT\", unknown_vals=[\"0\", \"na\", \"n/a\", \"-\", \"null\", \"nan\", \"\", \"nil\", \"nil.\"])\n",
    "data[\"IMPLANT\"] = (\n",
    "    data[\"IMPLANT\"]\n",
    "    .astype(str)\n",
    "    .str.strip(\" ;,.-\")\n",
    "    .str.replace(r\"\\bx\\d+\\b\", \"\", regex=True)\n",
    "    .str.replace(r\"\\byes\\b\", \"\", regex=True)\n",
    "    .str.replace(r\"\\s+\", \" \", regex=True)    \n",
    "    .str.strip()\n",
    "    .str.replace(\"&\", \"and\", regex=False)\n",
    ")\n",
    "inspect_column(data, \"IMPLANT\", top_n=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect DIAGNOSIS\n",
    "just normal standardisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_column(data, \"DIAGNOSIS\", top_n=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = normalize_text(data, \"DIAGNOSIS\", unknown_vals=[\"0\", \"na\", \"n/a\", \"-\", \"null\", \"nan\", \"\", \"nil\"])\n",
    "inspect_column(data, \"DIAGNOSIS\", top_n=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect CANCER_INDICATOR\n",
    "just normal standardisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_column(data, \"CANCER_INDICATOR\", top_n=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = normalize_text(data, \"CANCER_INDICATOR\", unknown_vals=[\"0\", \"na\", \"n/a\", \"-\", \"null\", \"nan\"])\n",
    "data = data[data[\"CANCER_INDICATOR\"].isin([\"false\", \"true\"])]\n",
    "inspect_column(data, \"CANCER_INDICATOR\", top_n=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect TUMOR_INDICATOR\n",
    "just normal standardisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_column(data, \"TRAUMA_INDICATOR\", top_n=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = normalize_text(data, \"TRAUMA_INDICATOR\", unknown_vals=[\"0\", \"na\", \"n/a\", \"-\", \"null\", \"nan\"])\n",
    "inspect_column(data, \"TRAUMA_INDICATOR\", top_n=30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect Delay_Reason\n",
    "No problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_column(data, \"Delay_Reason\", top_n=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect Remarks\n",
    "No problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_column(data, \"Remarks\", top_n=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save file from cleaning steps above into a seperate file (change file name if required)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"oots-data-cleaning-1.xlsx\"\n",
    "data.to_excel(output_path, index=False)\n",
    "\n",
    "print(f\"Saved to {output_path}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EMMAS PART"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the highest frequency of words, bigrams, and trigrams to be used in taxonomy for categorisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"Delay_Reason\" not in data.columns:\n",
    "    raise KeyError(f\"'Delay Reason' not found. Available columns: {list(data.columns)}\")\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize text in Delay_Reason (remove punctuation, standardise case, remove trialing spaces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_punct_tbl = str.maketrans(\"\", \"\", string.punctuation)\n",
    "\n",
    "def normalize_text(s: str) -> str:\n",
    "\n",
    "    s = str(s).lower()\n",
    "    s = s.translate(_punct_tbl)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "\n",
    "    s = re.sub(r\"\\bo\\.t\\b\", \"operating theater\", s)\n",
    "    s = re.sub(r\"\\bot\\b\", \"operating theater\", s)\n",
    "    s = re.sub(r\"\\bo\\.r\\b\", \"operating room\", s)\n",
    "    s = re.sub(r\"\\banaesth\\b\", \"anaesthesia\", s)\n",
    "    s = re.sub(r\"\\banesth\\b\", \"anaesthesia\", s)\n",
    "    s = re.sub(r\"\\bpt\\b\", \"patient\", s)\n",
    "    s = re.sub(r\"\\bprev\\b\", \"previous\", s)\n",
    "    s = re.sub(r\"\\bdr\\b\", \"doctor\", s)\n",
    "    s = re.sub(r\"\\bpre-med\\b\", \"premedication\", s)\n",
    "\n",
    "    return s\n",
    "\n",
    "# Apply normalization\n",
    "data[\"_Delay_norm\"] = data[\"Delay_Reason\"].astype(str).fillna(\"\").map(normalize_text)\n",
    "data[[\"_Delay_norm\"]].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = {\n",
    "    \"the\",\"a\",\"an\",\"and\",\"or\",\"to\",\"of\",\"for\",\"by\",\"with\",\"from\",\n",
    "    \"is\",\"are\",\"was\",\"were\",\"be\",\"been\",\"being\",\"due\",\"because\",\n",
    "    \"this\",\"that\",\"it\",\"as\",\"into\",\"per\",\"via\", \"eg\", \"etc\"\n",
    "}\n",
    "\n",
    "# Initialize containers\n",
    "words, bigrams, trigrams = [], [], []\n",
    "\n",
    "# Tokenize each delay reason\n",
    "for text in data[\"_Delay_norm\"]:\n",
    "    tokens = [t for t in text.split() if t and t not in STOPWORDS]\n",
    "    if not tokens:\n",
    "        continue\n",
    "\n",
    "    words.extend(tokens)\n",
    "    if len(tokens) >= 2:\n",
    "        bigrams.extend([\" \".join(tokens[i:i+2]) for i in range(len(tokens)-1)])\n",
    "    if len(tokens) >= 3:\n",
    "        trigrams.extend([\" \".join(tokens[i:i+3]) for i in range(len(tokens)-2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_FILE = \"oots-data-cleaning-3-flagged.xlsx\"\n",
    "OUTPUT_CSV  = \"oots-data-cleaning-3-flagged.csv\"\n",
    "\n",
    "COL = \"Delay_Reason\"\n",
    "s = data[COL].astype(str)\n",
    "\n",
    "clean = (\n",
    "    s.str.lower()\n",
    "     .str.replace(r\"[^\\w\\s]\", \"\", regex=True)    \n",
    "     .str.replace(r\"\\s+\", \" \", regex=True)       \n",
    "     .str.strip()\n",
    ")\n",
    "\n",
    "raw = s.str.strip()\n",
    "only_punct_or_numbers = raw.str.match(r'^(?=.*\\S)(?!.*[A-Za-z]).*$', na=False)\n",
    "\n",
    "data.loc[only_punct_or_numbers, COL] = \"0\"\n",
    "\n",
    "not_late_phrases = [\n",
    "    \"no delay\", \"not delay\", \"not delayed\", \"not late\",\n",
    "    \"na\", \"0\", \"null\", \"nan\"\n",
    "]\n",
    "\n",
    "def phrase_to_token_pattern(p: str) -> str:\n",
    "    p = p.strip().lower()\n",
    "    esc = re.escape(p).replace(r\"\\ \", r\"\\s+\")\n",
    "    return rf\"(?<!\\w){esc}(?!\\w)\"\n",
    "\n",
    "pattern = r\"(?:{})\".format(\"|\".join(phrase_to_token_pattern(p) for p in not_late_phrases))\n",
    "regex = re.compile(pattern, flags=re.IGNORECASE)\n",
    "\n",
    "phrase_hit = clean.str.contains(regex, na=False)\n",
    "\n",
    "data[\"Reason_Is_Late\"] = np.where(only_punct_or_numbers | phrase_hit, 0, 1)\n",
    "\n",
    "data[COL] = clean\n",
    "data.loc[only_punct_or_numbers, COL] = \"0\"\n",
    "data.drop(columns=COL, inplace=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding target variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Surgery duration (knife → closure) ---\n",
    "data[\"ACTUAL_SURGERY_DURATION\"]  = data[\"ACTUAL_SKIN_CLOSURE\"]  - data[\"ACTUAL_KNIFE_TO_SKIN_TIME\"]\n",
    "data[\"PLANNED_SURGERY_DURATION\"] = data[\"PLANNED_SKIN_CLOSURE\"] - data[\"PLANNED_KNIFE_TO_SKIN_TIME\"]\n",
    "data[\"DIFF_SURGERY_DURATION\"]    = data[\"ACTUAL_SURGERY_DURATION\"] - data[\"PLANNED_SURGERY_DURATION\"]\n",
    "\n",
    "# --- OR usage duration (enter OR → exit OR) ---\n",
    "data[\"ACTUAL_USAGE_DURATION\"]  = data[\"ACTUAL_EXIT_OR_TIME\"]  - data[\"ACTUAL_ENTER_OR_TIME\"]\n",
    "data[\"PLANNED_USAGE_DURATION\"] = data[\"PLANNED_EXIT_OR_TIME\"] - data[\"PLANNED_ENTER_OR_TIME\"]\n",
    "data[\"DIFF_USAGE_DURATION\"]    = data[\"ACTUAL_USAGE_DURATION\"] - data[\"PLANNED_USAGE_DURATION\"]\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "converting new target variables to minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_min = lambda s: s.dt.total_seconds() / 60\n",
    "for col in [\n",
    "    \"ACTUAL_SURGERY_DURATION\",\"PLANNED_SURGERY_DURATION\",\"DIFF_SURGERY_DURATION\",\n",
    "    \"ACTUAL_USAGE_DURATION\",\"PLANNED_USAGE_DURATION\",\"DIFF_USAGE_DURATION\",\n",
    "]:\n",
    "    data[col] = to_min(data[col])\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
